{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amyth18/CS598-Deep-Learning-Final-Project/blob/main/CS598_Deep_Learning_For_Healthcare_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHtTRNjY6fjL",
        "outputId": "2788a4dc-b052-4abb-807c-b9247fe69bec"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch"
      ],
      "metadata": {
        "id": "yZM4PBDMRCc_"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFlP8qR9EAEW",
        "outputId": "e4c24489-7afb-407d-eb79-3ead8c2d2c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZrO8Ao004k3"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcubO8rgEqXh",
        "outputId": "5b96de3b-578c-436e-d9c9-9bc620b5f6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "# read data\n",
        "df = pd.read_csv(\"/content/drive/My Drive/DLH Final Project/mimic3/NOTEEVENTS.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASN9TUHWgnz7"
      },
      "source": [
        "# Data Preprocessing\n",
        "Need to focus on 2 tables\n",
        "1. NOTESEVENTS.csv\n",
        "2. DIAGNOSES_ICD.csv\n",
        "\n",
        "Join tables by <subject_id, hadm_id>\n",
        "\n",
        "Construct 2 datasets from \"TEXT\" field in NOTESEVENTS.csv for each <subject_id, hadm_id> pair (i.e discharge summary for that admission.)\n",
        "\n",
        "X1, y and X2, y\n",
        "x1 = sequence of vectors from word2vec \n",
        "x2 = sequence of vectors from tf-idf\n",
        "y = list of icd codes for <subject_id, hadm_id> i.e. diagnosis maded in ICU admission.\n",
        "\n",
        "Need to focus on 50 and 100 most commonly diagnosed diseases.\n",
        "\n",
        "Use NLTK + MetaMap to extract only the symptom related entities (how to use MetaMap is unknown atm.)\n",
        "\n",
        "Filter out sections in discharge summaries that are related to symptoms only, ignore others to speed up things.\n",
        "\n",
        "Negative filters (e.g. \"no sign of breath problem\").\n",
        "\n",
        "Generate Word2Vec embeddings (currently using Gensim) using \"TEXT\".\n",
        "\n",
        "Generate TF-IDF vector for each symptom entity.\n",
        "\n",
        "Generate multi-hot encoding for y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Routines For Data Processing"
      ],
      "metadata": {
        "id": "kxdU3qT1k3m7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "TV7TblwfIxta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978caf11-cd5e-4320-88c6-d5bfd418147e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "eng_stop_words =  stopwords.words('english')\n",
        "\n",
        "class MySentences(object):\n",
        "    def __init__(self, dframe):\n",
        "        self.dframe = dframe\n",
        "    \n",
        "    # TODO: Keeping only alpha numeric characters and spaces for now.\n",
        "    # need to make this better. Find some good libraries.\n",
        "    def sanitize_text(self, text):\n",
        "      test = text.strip()\n",
        "      text = re.sub(r'\\s\\s+', ' ', text)\n",
        "      text = re.sub(r'[^a-zA-z0-9\\/\\.\\?\\!\\s;,\\'\\-]', '', text)\n",
        "      text = re.sub(r'[\\.\\-\\/\\?\\!;,]', ' ', text)\n",
        "      text = re.sub(r'[\\[\\]]', '', text)\n",
        "      return text\n",
        "\n",
        "    # TODO: adding some basic checks again need to make it better.\n",
        "    def sanitize_words(self, sentence):\n",
        "      return [w for w in sentence if w not in eng_stop_words and not w.isdigit()]\n",
        "\n",
        "    def __iter__(self):\n",
        "        for idx in range(len(self.dframe)):\n",
        "          text = self.sanitize_text(self.dframe[\"TEXT\"][idx])\n",
        "          yield self.sanitize_words(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_dataset(dataset):\n",
        "  seq_lengths = list()\n",
        "\n",
        "  for idx in range(len(dataset)):\n",
        "    seq_lengths.append(len(dataset[idx]))\n",
        "  max_seq_length = max(seq_lengths)\n",
        "  \n",
        "  padded_dataset = torch.zeros([len(dataset), max_seq_length, len(dataset[0][0])], \n",
        "                               dtype=torch.float64)\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      padded_dataset[i][j] = torch.FloatTensor(dataset[i][j])\n",
        "  \n",
        "  return padded_dataset"
      ],
      "metadata": {
        "id": "81XPts3oIlGR"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Filtering and Tranformation"
      ],
      "metadata": {
        "id": "n1xDk_pMlBPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_icd_codes = pd.read_csv(\n",
        "    \"/content/drive/My Drive/DLH Final Project/mimic3/DIAGNOSES_ICD.csv\")"
      ],
      "metadata": {
        "id": "0DQMAoiAPJyh"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get top #50 ICD9 codes "
      ],
      "metadata": {
        "id": "wIKf6j0HArw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = df_icd_codes[\"ICD9_CODE\"].value_counts().head(50)\n",
        "top_icd_codes = counts.index.to_list()"
      ],
      "metadata": {
        "id": "jNY4o2-_PsJD"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter data to include admimission with top 50 diseases only and group and reorganize data in the following format <subject_id, hadm_id, [icd_code1, icd_code2 ...]>"
      ],
      "metadata": {
        "id": "3wSpzZYNpeIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_admissions_with_top_diseases = \\\n",
        "df_icd_codes[df_icd_codes[\"ICD9_CODE\"].isin(top_icd_codes)]\n",
        "\n",
        "df_admissions_with_top_diseases = \\\n",
        "df_admissions_with_top_diseases.groupby(\n",
        "['SUBJECT_ID', 'HADM_ID'])['ICD9_CODE'].apply(\n",
        "        list).to_frame().reset_index()"
      ],
      "metadata": {
        "id": "zCBDPURVB2C5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now select discharge summaries for the admimissions that contain atleast one of the top #50 ICD codes."
      ],
      "metadata": {
        "id": "5lLEQXITBIyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset = pd.merge(df, df_admissions_with_top_diseases, \n",
        "                       on=[\"SUBJECT_ID\", \"HADM_ID\"])\n",
        "\n",
        "df_dataset = df_dataset[df_dataset[\"CATEGORY\"] \n",
        "                          == 'Discharge summary'].reset_index()\n",
        "# free up some memory\n",
        "# del df"
      ],
      "metadata": {
        "id": "BGIpZW0wfKaO"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJmeYMT3xiTE",
        "outputId": "0310813a-62d1-4705-8ee3-801fc35e952e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55988"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to covert the ICD9 column to multi-hot encoding, we keep the old column with list of codes and added and new column with multi-hot encoding representation."
      ],
      "metadata": {
        "id": "hj5A2EJRqEzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_top_icd_codes = sorted(top_icd_codes)\n",
        "icd_code_to_idx = dict((k, v) for v, k in enumerate(sorted_top_icd_codes))"
      ],
      "metadata": {
        "id": "n-MxiQuQTZWm"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new col to be added to dataframe\n",
        "multi_hot_ecoding_col = list()\n",
        "for idx in range(len(df_dataset)):\n",
        "  icd_codes = df_dataset.iloc[idx]['ICD9_CODE']\n",
        "  encoding = [0] * 50\n",
        "  for code in icd_codes:\n",
        "    encoding[icd_code_to_idx[code]] = 1    \n",
        "  multi_hot_ecoding_col.append(encoding)\n",
        "\n",
        "# new add a new column with multi-hot encoding.\n",
        "df_dataset['ICD9_CODE_ENCODED'] = multi_hot_ecoding_col"
      ],
      "metadata": {
        "id": "S7vIzsO9V-63"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract symptoms from the text. Note: currently we treat all tokens as symptoms need to add all the filters discussed in the paper later. So we added a column called \"SYMPTOMS\" which is simply tokenized form of \"TEXT\" after basic sanitization."
      ],
      "metadata": {
        "id": "rAtNNVUmCdw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgen = MySentences(df_dataset)\n",
        "symptom_col = list()\n",
        "for s in sgen:\n",
        "  symptom_col.append(s)\n",
        "\n",
        "# add the new column to the dataset.\n",
        "df_dataset[\"SYMPTOMS\"] = symptom_col"
      ],
      "metadata": {
        "id": "GFO96uEPsVC6"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Word2Vec Embeddings"
      ],
      "metadata": {
        "id": "RIwaO1C1td27"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg-GmTrg0i6h"
      },
      "source": [
        "Word2Vec training using gensim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "xElMsqPx0sc9"
      },
      "outputs": [],
      "source": [
        "# NOTE: commenting this part so that we dont run this by mistake.\n",
        "\n",
        "# import gensim\n",
        "# sgen = MySentences(df_dataset) # a memory-friendly iterator\n",
        "# model = gensim.models.Word2Vec(sgen, min_count=5, workers=4, sample=1e-05)\n",
        "# model.save(\"/content/drive/My Drive/DLH Final Project/mimic3/word2vec-4.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct dataset with Word2Vec embeddings"
      ],
      "metadata": {
        "id": "crLZigFVcbeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load('/content/drive/My Drive/DLH Final Project/mimic3/word2vec-4.model')"
      ],
      "metadata": {
        "id": "CE5ABBKOcw21"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_word2vec = list()\n",
        "for idx in range(len(df_dataset)):\n",
        "  # ignore words in not vocabulary\n",
        "  symptoms = df_dataset[\"SYMPTOMS\"][idx]\n",
        "  symptoms_emb = [model.wv[s] for s in symptoms if s in model.wv]\n",
        "  X_word2vec.append(symptoms_emb)\n",
        "\n",
        "# pad the dataset.\n",
        "# X_word2vec = pad_dataset(X_word2vec)"
      ],
      "metadata": {
        "id": "YQu3AEBEfbF3"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# pfile = open(\"/content/drive/My Drive/DLH Final Project/X_word2vec\", \"ab\")\n",
        "# pickle.dump(X_word2vec, pfile)\n",
        "# pfile.close()"
      ],
      "metadata": {
        "id": "QTY-rPP6l2zJ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct data with TF-IDF encoding"
      ],
      "metadata": {
        "id": "V8Q5MAs2xjDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "vocab_size = len(model.wv)\n",
        "tf = np.zeros((len(model.wv), len(top_icd_codes)))\n",
        "\n",
        "\n",
        "for idx in range(len(df_dataset)):\n",
        "  # XXX: TODO currently we treat all tokens from \"TEXT\" as sypmtoms\n",
        "  # get the icd codes for this discharge summary\n",
        "  symptoms = df_dataset['SYMPTOMS'][idx]\n",
        "  icd_codes = df_dataset['ICD9_CODE'][idx]\n",
        "  # create a cross product of symptoms and icd codes\n",
        "  # and update tf matrix. tf matrix keeps count of how many \n",
        "  # (i.e frequency) times <symptom, icd code> pair occur in our dataset.\n",
        "  for pair in itertools.product(symptoms, icd_codes):\n",
        "    # update count of each (symptom, icd_code) pair to compute TF\n",
        "    if pair[0] in model.wv:\n",
        "      tf[model.wv.get_index(pair[0])][icd_code_to_idx[pair[1]]] += 1\n",
        "\n",
        "# Complete the TF-IDF matrix computation.\n",
        "# Compute the number of ICD Codes (i.e diseaes) each \n",
        "# symptom is associated with.\n",
        "D_i = np.sum(tf > 0, axis=1)\n",
        "print(D_i.shape)\n",
        "\n",
        "log_N_Di = np.log(len(top_icd_codes)/D_i)\n",
        "tf_idf = (tf.T * log_N_Di).T\n",
        "print(tf_idf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOGw0qxSxrZ3",
        "outputId": "94e66315-7a79-42d5-f707-a5b56a8e699c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64259,)\n",
            "(64259, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the X_tfidf dataset\n",
        "X_tf_idf = list()\n",
        "for idx in range(len(df_dataset)):\n",
        "  symptoms = df_dataset[\"SYMPTOMS\"][idx]\n",
        "  # get tf-idf vector for each symptom\n",
        "  # ignore words in not vocabulary\n",
        "  symptoms_tf_idf = [tf_idf[model.wv.get_index(s)] \\\n",
        "                     for s in symptoms if s in model.wv]\n",
        "  X_tf_idf.append(symptoms_tf_idf)\n",
        "\n",
        "# pad the dataset.\n",
        "# X_tf_idf = pad_dataset(X_tf_idf)"
      ],
      "metadata": {
        "id": "W-9C5XBuiOJ3"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# pfile = open(\"/content/drive/My Drive/DLH Final Project/X_tf_idf\", \"ab\")\n",
        "# pickle.dump(tf_idf, pfile)\n",
        "# pfile.close()"
      ],
      "metadata": {
        "id": "E3afwJciiW59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct Y (Multihot Encoding)"
      ],
      "metadata": {
        "id": "ckHf55_ylpmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-hot encoding for ICD codes diagnosed.\n",
        "y = df_dataset['ICD9_CODE_ENCODED'].to_list()"
      ],
      "metadata": {
        "id": "qwrW5WOPkBPd"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_word2vec))\n",
        "print(len((X_tf_idf)))\n",
        "print(len(y))"
      ],
      "metadata": {
        "id": "CP4e2hJU4S6B",
        "outputId": "8715516e-e57b-47e0-96b7-cc8da7ebc7ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55988\n",
            "55988\n",
            "55988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "D0madEdCl7w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(data):\n",
        "  x_w2v, x_tidf, y_batch = zip(*data)\n",
        "  x_w2v = pad_dataset(x_w2v)\n",
        "  x_tidf = pad_dataset(x_tidf)\n",
        "  y_batch = torch.FloatTensor(y_batch)\n",
        "  # print(f\"{x_w2v.shape}, {x_tidf.shape}, {y_batch.shape}\")\n",
        "  return x_w2v, x_tidf, y_batch\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X_w2v, X_tfidf, y):              \n",
        "    self.X_w2v = X_w2v\n",
        "    self.X_tfidf = X_tfidf\n",
        "    self.y = y\n",
        "    \n",
        "  def __len__(self):                \n",
        "    return len(self.y)\n",
        "    \n",
        "  def __getitem__(self, index):          \n",
        "    # your code here\n",
        "    return self.X_w2v[index], self.X_tfidf[index], self.y[index]\n",
        "\n",
        "dataset = CustomDataset(X_word2vec, X_tf_idf, y)\n",
        "\n",
        "split = int(len(dataset)*0.8)\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, test_dataset = random_split(dataset, lengths)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32, \n",
        "                          collate_fn=collate_fn)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=32, \n",
        "                         collate_fn=collate_fn)\n",
        "\n",
        "a = iter(train_loader)\n",
        "p, q, r = next(a)\n",
        "print(type(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5HoLixIdZq_",
        "outputId": "0bd451d3-20d1-4710-f069-f84fedee1130"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1774, 100]), torch.Size([32, 1774, 50]), torch.Size([32, 50])\n",
            "<class 'NoneType'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "yilnzXZgi8RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, output_dim):   \n",
        "    super(BiLSTM, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size=input_dim, \n",
        "                        hidden_size=embedding_dim,\n",
        "                        num_layers=2,\n",
        "                        bidirectional=True,\n",
        "                        batch_first=True)\n",
        "    \n",
        "    self.linear = nn.Linear(embedding_dim*2, \n",
        "                            output_dim)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    emb = self.lstm(X)\n",
        "    output = F.sigmoid(self.linear(emb))\n",
        "    return output"
      ],
      "metadata": {
        "id": "RpByc-wrjQ6A"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiseasePredictionModel(nn.Module):\n",
        "  def __init__(self, weight=0.4):    \n",
        "    super(DiseasePredictionModel, self).__init__()\n",
        "    self.weight = 0.4    \n",
        "    self.w2v_lstm = BiLSTM(input_dim=100, embedding_dim=50, output_dim=50)\n",
        "    self.tf_idf_lstm = BiLSTM(input_dim=50, embedding_dim=50, output_dim=50)\n",
        "  \n",
        "  def forward(self, X_w2v, X_tidf):\n",
        "    pred1 = self.w2v_lstm(X_w2v)\n",
        "    pred2 = self.tf_idf_lstm(X_tidf)\n",
        "    # compute the weighted average of predictions\n",
        "    # from the 2 models.\n",
        "    return self.weight * pred1 + (1-self.weight) * pred2"
      ],
      "metadata": {
        "id": "z6v9DR1lm34D"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "wT5_eTcAfyzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiseasePredictionModel()\n",
        "loss = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for e in range(1):\n",
        "  model.train()\n",
        "  for x_w2v, x_tidf, y_batch in train_loader:    \n",
        "    model.zero_grad()\n",
        "    pred = model(x_w2v, x_tidf)\n",
        "    print(pred)\n",
        "    l = loss(pred, y_batch[idx])\n",
        "    print(loss)\n",
        "    l.backward() \n",
        "    optimizer.step()\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "djf3IOU68k9G",
        "outputId": "44f385d0-d879-405c-c51d-62589c4dfac0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-9ccdd8388175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-0eca76a3f0bb>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mx_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{x_w2v.shape}, {x_tfidf.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mx_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_w2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mx_tidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS598 Deep Learning For Healthcare Final Project",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}