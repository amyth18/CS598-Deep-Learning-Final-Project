{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Other Baseline Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyPgEZw+4HuGg51Z0EnJTFyl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amyth18/CS598-Deep-Learning-Final-Project/blob/main/Other_Baseline_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "In this notebook we provide the implementation for DeepLabeler model as proposed in the paper [Automated ICD-9 Coding via A Deep Learning Approach.](https://ieeexplore.ieee.org/document/8320340) by Min Li et al. The model makes disease prediction by using all of the clinical text (using Doc2Vec) as input to a Convolutional Neural Network. We use this model as one of baseline models to compare our main model implemented in [Main_Model.ipynb](https://github.com/amyth18/CS598-Deep-Learning-Final-Project/blob/main/Main_Model.ipynb)."
      ],
      "metadata": {
        "id": "pBUb6RByTug-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Requisites\n",
        "\n",
        "Before you can run this notebook you need to gain access to MIMIC III version 1.4 dataset from physionet.org, please refer to Pre-Requisites section in the Readme file in the GitHub repository for more details.\n",
        "\n",
        "For this notebook we specifically we need the `NOTEVENTS.csv` and `DIAGNOSES_ICD.csv` files from the MIMIC III dataset.\n",
        "\n",
        "We use Google Drive to store all our data including the original dataset, transformed/pre-processed dataset, trained models and evaluation results. Before you get started please create a top level folder in your Google Drive and update the project level env variable ```PROJECT_PATH``` in the **Intial Setup** section.\n",
        "\n",
        "Once the top level folder is created, please save the MIMIC III dataset (i.e the de-compressed csv files) in a folder called ```mimic3```. If you don't have space to save all the files, you can only save the `NOTEVENTS.csv` and `DIAGNOSES_ICD.csv`files.\n",
        "\n",
        "Also, in the same top level folder create the following folders where the notebook will save various results.\n",
        "1. ```models```\n",
        "2. ```results```\n",
        "3. ```stats```\n",
        "\n",
        "**IMPORTANT NOTE:** Before running this notebook, you will need to run the [Misc_Utilities.ipynb](https://github.com/amyth18/CS598-Deep-Learning-Final-Project/blob/main/Misc_Utilities.ipynb) notebook that does from data pre-processing and generates a file ```df_dataset_full_text_d2v.csv``` which serves as input to this notebook. We could not perform those data processing tasks in this notebook because of the python package conflicts."
      ],
      "metadata": {
        "id": "8zQu4CbfVLcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "-vFEFxSyVmFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i_en9fHp43m"
      },
      "outputs": [],
      "source": [
        "! pip install gensim --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJNa7v3lsRoA",
        "outputId": "f054e08a-0afb-42d7-df88-fd35b40d116b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Yg41pDraqbIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_PATH = \"/content/drive/My Drive/DLH Final Project\"\n",
        "DATASET_PATH = f\"{PROJECT_PATH}/mimic3/df_dataset_full_text.csv\"\n",
        "DATASET_D2V_PATH = f\"{PROJECT_PATH}/mimic3/df_dataset_full_text_d2v.csv\"\n",
        "DOC2VEC_PATH = f\"{PROJECT_PATH}/models/doc2vec.model\"\n",
        "W2V_MODEL_PATH = f\"{PROJECT_PATH}/models/word2vec.model\"\n",
        "\n",
        "TRAINING_BATCH_SIZE = 400\n",
        "MAX_WORDS = 1000\n",
        "W2V_EMB_SIZE = 128"
      ],
      "metadata": {
        "id": "lGz7aVYrqvJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls \"/content/drive/My Drive/DLH Final Project/models\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F57y2QCLsIEq",
        "outputId": "3ca1743f-47cb-43d8-f843-ae1233f5e8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc2vec.model\t\t\tmain-model-27-04-2022-19-11-16\n",
            "doc2vec.model.syn1neg.npy\ttf-idf-27-04-2022-16-29-54\n",
            "doc2vec.model.wv.vectors.npy\tword2vec-27-04-2022-17-47-59\n",
            "main-model-27-04-2022-15-40-59\tword2vec.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "PuUIA2ZgBHuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset = pd.read_csv(DATASET_PATH, converters={'INPUT_TEXT': eval, \n",
        "                                                   'ICD9_CODE': eval})"
      ],
      "metadata": {
        "id": "ivuKcLL6qYzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from os import path\n",
        "\n",
        "model = None\n",
        "\n",
        "if path.exists(DOC2VEC_PATH):\n",
        "  print(\"Loading doc2vec model from disk.\")\n",
        "  model = Doc2Vec.load(DOC2VEC_PATH)\n",
        "else:\n",
        "  print(\"Building doc2vec model.\")\n",
        "  docs = [TaggedDocument(doc, [i])\n",
        "          for i, doc in enumerate(df_dataset.INPUT_TEXT)]\n",
        "\n",
        "  model = Doc2Vec(vector_size=128, window=2, \n",
        "                  min_count=1, \n",
        "                  workers=8, \n",
        "                  epochs = 40)\n",
        "\n",
        "  model.build_vocab(docs)\n",
        "\n",
        "  model.train(docs, total_examples=model.corpus_count, \n",
        "              epochs=model.epochs)\n",
        "  model.save(DOC2VEC_PATH)"
      ],
      "metadata": {
        "id": "2z6DqEnOt08a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_doc2vec = [model.infer_vector(df_dataset['INPUT_TEXT'][i]) \n",
        "              for i in range(0, len(df_dataset['INPUT_TEXT']))]"
      ],
      "metadata": {
        "id": "H_IA5Nm1vz2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset['DOC2VEC'] = np.array(X_doc2vec).tolist()\n",
        "df_dataset.to_csv(DATASET_D2V_PATH)"
      ],
      "metadata": {
        "id": "TAlz7CrBEAUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "9reTI7ddGolP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset = pd.read_csv(DATASET_D2V_PATH, \n",
        "                         converters={'INPUT_TEXT': eval, \n",
        "                                     'ICD9_CODE': eval,\n",
        "                                     'DOC2VEC': eval})"
      ],
      "metadata": {
        "id": "Eb2F4G2eG20U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# load the model\n",
        "model = Word2Vec.load(W2V_MODEL_PATH)\n",
        "\n",
        "# now create a vector of word2vec embeddings for each discharge summary\n",
        "X_word2vec = list()\n",
        "for idx in range(len(df_dataset)):\n",
        "  # ignore words in not vocabulary\n",
        "  text = df_dataset[\"INPUT_TEXT\"][idx]\n",
        "  word_emb = [model.wv[w] for w in text if w in model.wv]\n",
        "  X_word2vec.append(word_emb)"
      ],
      "metadata": {
        "id": "L05N6L2f9E8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top 50 unique ICD codes.\n",
        "top_icd_codes = [codes for codes in df_dataset['ICD9_CODE']]\n",
        "top_icd_codes = np.unique([code for codes in top_icd_codes for code in codes])\n",
        "\n",
        "sorted_top_icd_codes = sorted(top_icd_codes)\n",
        "icd_code_to_idx = dict((k, v) for v, k in enumerate(sorted_top_icd_codes))\n",
        "\n",
        "multi_hot_ecoding_col = list()\n",
        "for idx in range(len(df_dataset)):\n",
        "  icd_codes = df_dataset.iloc[idx]['ICD9_CODE']\n",
        "  encoding = [0] * 50\n",
        "  for code in icd_codes:\n",
        "    encoding[icd_code_to_idx[code]] = 1    \n",
        "  multi_hot_ecoding_col.append(encoding)\n",
        "\n",
        "# new add a new column with multi-hot encoding.\n",
        "df_dataset['ICD9_CODE_ENCODED'] = multi_hot_ecoding_col\n",
        "\n",
        "# multi-hot encoding for ICD codes diagnosed.\n",
        "y = df_dataset['ICD9_CODE_ENCODED'].to_list()"
      ],
      "metadata": {
        "id": "deKr44cawezx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_doc2vec = df_dataset[\"DOC2VEC\"]"
      ],
      "metadata": {
        "id": "LY8vi4oQXVld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_word2vec))\n",
        "print(len(X_doc2vec))\n",
        "print(len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05VFq48T_7b2",
        "outputId": "2bb14870-d35d-4479-b7a3-3c2efd275596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55988\n",
            "55988\n",
            "55988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets and Dataloaders"
      ],
      "metadata": {
        "id": "PeBZno_f-Usd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "q7u-eGRx-2XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_dataset(dataset, vec_size):\n",
        "  seq_lengths = list()\n",
        "\n",
        "  # for idx in range(len(dataset)):\n",
        "  #   seq_lengths.append(len(dataset[idx]))\n",
        "  # max_seq_length = max(seq_lengths)\n",
        "\n",
        "  padded_dataset = torch.zeros([len(dataset), MAX_WORDS, vec_size], \n",
        "                               dtype=torch.float)\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      padded_dataset[i][j] = torch.FloatTensor(dataset[i][j])\n",
        "  \n",
        "  return padded_dataset"
      ],
      "metadata": {
        "id": "nWxMgdES-wg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def collate_fn(data):\n",
        "  x_w2v, x_d2v, y_batch = zip(*data)  \n",
        "  x_w2v = pad_dataset(x_w2v, W2V_EMB_SIZE)  \n",
        "  x_d2v = torch.FloatTensor(x_d2v)\n",
        "  y_batch = torch.FloatTensor(y_batch)\n",
        "  # move to gpus\n",
        "  x_w2v = x_w2v.cuda() if torch.cuda.is_available() else x_w2v\n",
        "  x_d2v = x_d2v.cuda() if torch.cuda.is_available() else x_d2v\n",
        "  y_batch = y_batch.cuda() if torch.cuda.is_available() else y_batch\n",
        "  return (x_w2v, x_d2v), y_batch"
      ],
      "metadata": {
        "id": "E9fc5q6m-zVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, X_w2v, X_d2v, y):              \n",
        "    self.X_w2v = X_w2v\n",
        "    self.X_d2v = X_d2v\n",
        "    self.y = y\n",
        "    \n",
        "  def __len__(self):                \n",
        "    return len(self.y)\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    return self.X_w2v[index], self.X_d2v[index], self.y[index]\n",
        "\n",
        "dataset = CustomDataset(X_word2vec, X_doc2vec, y)\n",
        "split = int(len(dataset)*0.8)\n",
        "lengths = [split, len(dataset) - split]\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, lengths)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, \n",
        "                          batch_size=TRAINING_BATCH_SIZE, \n",
        "                          collate_fn=collate_fn)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, shuffle=True, \n",
        "                         batch_size=TRAINING_BATCH_SIZE, \n",
        "                         collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "hOa5r2Em-Ypb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "yC6wO1jJA2-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(CNNModel, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, (5, 128), 1)\n",
        "    self.max_pool = torch.nn.MaxPool2d(4)\n",
        "    self.dropout = torch.nn.Dropout(0.75)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "  \n",
        "  def forward(self, X):\n",
        "    out = self.conv1(X)\n",
        "    # print(out.shape)\n",
        "    out = self.relu(out)\n",
        "    x_in = torch.squeeze(out, dim=3)\n",
        "    # print(x_in.shape)\n",
        "    out = self.max_pool(x_in)\n",
        "    out = self.dropout(out)\n",
        "    # print(out.shape)\n",
        "    out = torch.flatten(out, 1)\n",
        "    return out\n",
        "\n",
        "class DeepLabeler(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(DeepLabeler, self).__init__()\n",
        "    self.cnn = CNNModel()    \n",
        "    self.fc = nn.Linear(4112, 50)\n",
        "    self.dropout = nn.Dropout(0.75)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, X_w2vec, X_d2vec):\n",
        "    out1 = self.cnn(X_w2vec)\n",
        "    # print(out1.shape)\n",
        "    # print(X_d2vec.shape)\n",
        "    X_concat = torch.cat((out1, X_d2vec), 1)\n",
        "    out2 = self.sigmoid(self.fc(X_concat))\n",
        "    # print(out2.shape)\n",
        "    return out2\n",
        "  \n",
        "  def get_name():\n",
        "    return \"deep-labeler\""
      ],
      "metadata": {
        "id": "GekrpFKWxB8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "waC5QEuaA84C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "def get_model_file_name(modelname=\"model\"):\n",
        "  return \"/content/drive/My Drive/DLH Final Project/models/\" + modelname + \"-\" + \\\n",
        "                  datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\n",
        "                      \"%d-%m-%Y-%H-%M-%S\")\n",
        "\n",
        "def get_stats_file_name(modelname=\"model\"):\n",
        "  return \"/content/drive/My Drive/DLH Final Project/stats/\" + modelname + \"-\" + \\\n",
        "                  datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\n",
        "                      \"%d-%m-%Y-%H-%M-%S\")\n",
        "\n",
        "def get_results_file_name(modelname=\"model\"):\n",
        "  return \"/content/drive/My Drive/DLH Final Project/results/\" + modelname + \\\n",
        "                  \"-\" + datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\n",
        "                      \"%d-%m-%Y-%H-%M-%S\")"
      ],
      "metadata": {
        "id": "v3jta40-C7xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "no_of_epocs = 100\n",
        "\n",
        "def train_model(model, loss, optimizer, train_loader):\n",
        "\n",
        "  main_memory_usage = list()\n",
        "  gpu_memory_usage = list()\n",
        "  gpu_time = list()\n",
        "  train_loss = list()\n",
        "\n",
        "  for e in range(no_of_epocs):\n",
        "    model.train()\n",
        "    epoc_train_loss = 0\n",
        "    main_memory_before = psutil.virtual_memory().used\n",
        "    gpu_memory_before = torch.cuda.memory_allocated()\n",
        "    start_time = time.time()\n",
        "\n",
        "    # iterate over data in mini batches.\n",
        "    for tup, y_batch in train_loader:\n",
        "      X_w2v, X_d2v = tup\n",
        "      X_w2v = torch.unsqueeze(X_w2v, dim=1)\n",
        "      model.zero_grad()\n",
        "      pred = model(X_w2v, X_d2v)\n",
        "      l = loss(pred, y_batch)\n",
        "      l.backward()\n",
        "      optimizer.step()    \n",
        "      epoc_train_loss += l.item()\n",
        "      \n",
        "    # print epoc level training loss.\n",
        "    print(f\"epoc: {e}: Train Loss: {epoc_train_loss/len(train_loader)}\")\n",
        "    \n",
        "    # collect cpu and memory stats.\n",
        "    memory_used = psutil.virtual_memory().used\n",
        "    gpu_memory_used = torch.cuda.memory_allocated()\n",
        "    run_time = time.time() - start_time\n",
        "    print(f\"time: {run_time} memory_used: {memory_used} gpu_memory_used: {gpu_memory_used}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    train_loss.append(epoc_train_loss/len(train_loader))\n",
        "    main_memory_usage.append(memory_used)\n",
        "    gpu_memory_usage.append(gpu_memory_used)\n",
        "    gpu_time.append(run_time)\n",
        "    # end of one epoc\n",
        "\n",
        "  # save the model\n",
        "  torch.save(model.state_dict(), get_model_file_name(model.get_name()))\n",
        "  # print and collect stats.\n",
        "  print(psutil.virtual_memory())\n",
        "\n",
        "  stats = {\n",
        "      \"gpu_mem\": gpu_memory_usage,\n",
        "      \"main_mem\": main_memory_usage,\n",
        "      \"gpu_time\": gpu_time,\n",
        "      \"vmm_info\": psutil.virtual_memory()\n",
        "  }\n",
        "\n",
        "  with open(get_stats_file_name(model.get_name()), \"ab\") as sfile:\n",
        "    pickle.dump(stats, sfile)"
      ],
      "metadata": {
        "id": "cSOc8usuC9Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DeepLabeler()\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(f\"No of parameters to train: \\\n",
        "        {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "train_model(model, loss_fn, optim, train_loader)"
      ],
      "metadata": {
        "id": "4ChBSS43DTIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(get_stats_file_name(\"deep-labeler\"), \"ab\") as sfile:\n",
        "    pickle.dump(model, sfile)"
      ],
      "metadata": {
        "id": "lSemGIpA22aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "rtLpiRHwA_jM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "  model.eval()\n",
        "  y_pred_all = list()\n",
        "  y_true_all = list()\n",
        "\n",
        "  for tup, y_batch in test_loader:\n",
        "    X_w2v, X_d2v = tup\n",
        "    X_w2v = torch.unsqueeze(X_w2v, dim=1)\n",
        "    y_pred = model(X_w2v, X_d2v)\n",
        "    y_pred = y_pred > 0.20 # TODO: remove hard coding\n",
        "    y_pred_all.extend(y_pred.detach().to('cpu').numpy())\n",
        "    y_true_all.extend(y_batch.detach().to('cpu').numpy())\n",
        "\n",
        "  y_true_all = np.array(y_true_all)\n",
        "  y_pred_all = np.array(y_pred_all)\n",
        "\n",
        "  # micro level metrics\n",
        "  p1, r1, f1, s1 = precision_recall_fscore_support(y_true_all, y_pred_all, \n",
        "                                                  average=\"micro\")\n",
        "  micro_auc = roc_auc_score(y_true_all, y_pred_all, average=\"micro\")\n",
        "  print(f\"Micro Averaging. Precision: {p1}, Recall: {r1}, F1 Score: {f1}, \\\n",
        "          AUC: {micro_auc}\")\n",
        "\n",
        "  # macro level metrics\n",
        "  p2, r2, f2, s2 = precision_recall_fscore_support(y_true_all, y_pred_all, \n",
        "                                                  average=\"macro\")\n",
        "  macro_auc = roc_auc_score(y_true_all, y_pred_all, average=\"macro\")\n",
        "  print(f\"Macro Averaging. Precision: {p2}, Recall: {r2}, F1 Score: {f2}, \\\n",
        "          AUC: {macro_auc}\")\n",
        "\n",
        "  results = {\n",
        "      \"micro\": [p1, r1, f1],\n",
        "      \"macro\": [p2, r2, f2]\n",
        "  }\n",
        "\n",
        "  with open(get_results_file_name(\"deep-labeler\"), \"ab\") as rfile:\n",
        "    pickle.dump(results, rfile)\n",
        "  \n",
        "  for idx in range(50):\n",
        "    p, r, f, _12 = precision_recall_fscore_support(y_true_all[:,idx], \n",
        "                                                 y_pred_all[:,idx], \n",
        "                                                 average='binary')\n",
        "    print(f\"p={p}, r={r}, f={f}\")"
      ],
      "metadata": {
        "id": "lEgWH_NqDD5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model is None:\n",
        "  print(\"load from disk\")\n",
        "  model = DeepLabeler()\n",
        "  if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    model.load_state_dict(torch.load(f\"{PROJECT_PATH}/models/\"))\n",
        "    evaluate_model(model, test_loader)\n",
        "else:\n",
        "  print(\"evaluating in-memory model\")\n",
        "  evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXDpBx8jDkNs",
        "outputId": "864e7354-c51a-40ba-f047-8d2c41daa07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating in-memory model\n",
            "Micro Averaging. Precision: 0.43531539678950937, Recall: 0.5684117299744145, F1 Score: 0.49303913618710254,           AUC: 0.7474107992817327\n",
            "Macro Averaging. Precision: 0.39562850169915237, Recall: 0.4724516202766275, F1 Score: 0.4190941059846913,           AUC: 0.6954960423444898\n",
            "p=0.37510729613733906, r=0.5421836228287841, f=0.443429731100964\n",
            "p=0.5836431226765799, r=0.5804066543438078, f=0.5820203892493049\n",
            "p=0.36137366099558915, r=0.5758032128514057, f=0.44405729771583435\n",
            "p=0.30420280186791193, r=0.3627684964200477, f=0.3309143686502177\n",
            "p=0.38059047062262497, r=0.6899841017488076, f=0.4905802562170309\n",
            "p=0.28212290502793297, r=0.2069672131147541, f=0.23877068557919623\n",
            "p=0.23865877712031558, r=0.18113772455089822, f=0.20595744680851066\n",
            "p=0.2871287128712871, r=0.3228744939271255, f=0.3039542639352072\n",
            "p=0.3746630727762803, r=0.29324894514767935, f=0.3289940828402367\n",
            "p=0.47126436781609193, r=0.5705765407554672, f=0.5161870503597122\n",
            "p=0.2181571815718157, r=0.13690476190476192, f=0.16823406478578895\n",
            "p=0.2783018867924528, r=0.17302052785923755, f=0.21338155515370705\n",
            "p=0.29739776951672864, r=0.22191400832177532, f=0.2541699761715648\n",
            "p=0.24956063268892795, r=0.1977715877437326, f=0.2206682206682207\n",
            "p=0.378125, r=0.2439516129032258, f=0.2965686274509804\n",
            "p=0.4975398313027179, r=0.9516020613936814, f=0.6534348796061236\n",
            "p=0.3594306049822064, r=0.4105691056910569, f=0.38330170777988615\n",
            "p=0.52, r=0.6179577464788732, f=0.5647626709573612\n",
            "p=0.3212121212121212, r=0.4602026049204052, f=0.3783462224866151\n",
            "p=0.2629032258064516, r=0.21391076115485563, f=0.23589001447178\n",
            "p=0.6016837674296238, r=0.8223660553757641, f=0.6949255545426922\n",
            "p=0.375, r=0.3140625, f=0.34183673469387754\n",
            "p=0.6182212581344902, r=0.504424778761062, f=0.5555555555555556\n",
            "p=0.5914074737077646, r=0.9139004149377593, f=0.7181089525879635\n",
            "p=0.2357142857142857, r=0.060218978102189784, f=0.09593023255813954\n",
            "p=0.5013736263736264, r=0.8522348232154769, f=0.6313318507536447\n",
            "p=0.29004854368932037, r=0.43933823529411764, f=0.34941520467836257\n",
            "p=0.23972602739726026, r=0.07918552036199095, f=0.11904761904761903\n",
            "p=0.23819978046103182, r=0.2265135699373695, f=0.23220973782771537\n",
            "p=0.4688995215311005, r=0.4661117717003567, f=0.46750149075730474\n",
            "p=0.23529411764705882, r=0.20880913539967375, f=0.2212618841832325\n",
            "p=0.4310405643738977, r=0.7474006116207951, f=0.5467561521252796\n",
            "p=0.30115361262902246, r=0.36072727272727273, f=0.32825943084050296\n",
            "p=0.29867674858223064, r=0.3191919191919192, f=0.30859375\n",
            "p=0.42005988023952096, r=0.7018509254627313, f=0.5255665855029032\n",
            "p=0.33825338253382536, r=0.36764705882352944, f=0.3523382447149263\n",
            "p=0.4211494252873563, r=0.6010498687664042, f=0.49526899161935656\n",
            "p=0.6996951219512195, r=0.936734693877551, f=0.8010471204188482\n",
            "p=0.4270986745213549, r=0.5291970802919708, f=0.4726976365118174\n",
            "p=0.44516653127538586, r=0.6773794808405439, f=0.5372549019607843\n",
            "p=0.34450651769087526, r=0.33575317604355714, f=0.34007352941176466\n",
            "p=0.5968882602545968, r=0.9440715883668904, f=0.731369150779896\n",
            "p=0.2237442922374429, r=0.08448275862068966, f=0.12265331664580725\n",
            "p=0.6671177266576455, r=0.9685658153241651, f=0.7900641025641025\n",
            "p=0.7524271844660194, r=0.8378378378378378, f=0.7928388746803069\n",
            "p=0.6530172413793104, r=0.9409937888198758, f=0.7709923664122138\n",
            "p=0.34615384615384615, r=0.3204334365325077, f=0.33279742765273307\n",
            "p=0.30326295585412666, r=0.26421404682274247, f=0.2823949955317247\n",
            "p=0.34297872340425534, r=0.5114213197969543, f=0.41059602649006627\n",
            "p=0.3320825515947467, r=0.33270676691729323, f=0.3323943661971831\n"
          ]
        }
      ]
    }
  ]
}