{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amyth18/CS598-Deep-Learning-Final-Project/blob/main/CS598_Deep_Learning_For_Healthcare_Final_Project_kbr_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHtTRNjY6fjL",
        "outputId": "52d14608-3128-44d3-f4fd-d9be4782a54f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyLQAuzpwLUZ",
        "outputId": "ebf569d5-13d4-42a1-990d-0e5426cd5cfc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch"
      ],
      "metadata": {
        "id": "yZM4PBDMRCc_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFlP8qR9EAEW",
        "outputId": "262d42e8-7fa5-4d5d-ab17-a4ae03017c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZrO8Ao004k3"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcubO8rgEqXh",
        "outputId": "212a2c4d-6146-4530-deb0-a2228a332b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "# read data\n",
        "df = pd.read_csv(\"/content/drive/My Drive/DLH Final Project/mimic3/NOTEEVENTS.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASN9TUHWgnz7"
      },
      "source": [
        "# Data Preprocessing\n",
        "Need to focus on 2 tables\n",
        "1. NOTESEVENTS.csv\n",
        "2. DIAGNOSES_ICD.csv\n",
        "\n",
        "Join tables by <subject_id, hadm_id>\n",
        "\n",
        "Construct 2 datasets from \"TEXT\" field in NOTESEVENTS.csv for each <subject_id, hadm_id> pair (i.e discharge summary for that admission.)\n",
        "\n",
        "X1, y and X2, y\n",
        "x1 = sequence of vectors from word2vec \n",
        "x2 = sequence of vectors from tf-idf\n",
        "y = list of icd codes for <subject_id, hadm_id> i.e. diagnosis maded in ICU admission.\n",
        "\n",
        "Need to focus on 50 and 100 most commonly diagnosed diseases.\n",
        "\n",
        "Use NLTK + MetaMap to extract only the symptom related entities (how to use MetaMap is unknown atm.)\n",
        "\n",
        "Filter out sections in discharge summaries that are related to symptoms only, ignore others to speed up things.\n",
        "\n",
        "Negative filters (e.g. \"no sign of breath problem\").\n",
        "\n",
        "Generate Word2Vec embeddings (currently using Gensim) using \"TEXT\".\n",
        "\n",
        "Generate TF-IDF vector for each symptom entity.\n",
        "\n",
        "Generate multi-hot encoding for y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Routines For Data Processing"
      ],
      "metadata": {
        "id": "kxdU3qT1k3m7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TV7TblwfIxta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb070c1-9236-4f17-c251-1bd93e51daf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "import re\n",
        "import nltk\n",
        "from transformers import AutoTokenizer, pipeline,  AutoModelForTokenClassification\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "eng_stop_words =  stopwords.words('english')\n",
        "\n",
        "class MySentences(object):\n",
        "    def __init__(self, dframe):\n",
        "        self.dframe = dframe\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"samrawal/bert-base-uncased_clinical-ner\")\n",
        "        model = AutoModelForTokenClassification.from_pretrained(\n",
        "            \"samrawal/bert-base-uncased_clinical-ner\")\n",
        "        symptom_extractor = pipeline('ner', model=model, tokenizer=tokenizer,\n",
        "                                     device=0)\n",
        "        text_anno = symptom_extractor(self.dframe[\"TEXT\"].tolist(), \n",
        "                                           batch_size=256)\n",
        "        self.data = list(zip(text_anno, self.dframe[\"TEXT\"].tolist()))\n",
        "        \n",
        "\n",
        "\n",
        "    \n",
        "    # TODO: Keeping only alpha numeric characters and spaces for now.\n",
        "    # need to make this better. Find some good libraries.\n",
        "    def sanitize_text(self, text):\n",
        "      test = text.strip()\n",
        "      text = re.sub(r'\\s\\s+', ' ', text)\n",
        "      text = re.sub(r'[^a-zA-z0-9\\/\\.\\?\\!\\s;,\\'\\-]', '', text)\n",
        "      text = re.sub(r'[\\.\\-\\/\\?\\!;,]', ' ', text)\n",
        "      text = re.sub(r'[\\[\\]]', '', text)\n",
        "      return text\n",
        "\n",
        "    def extract_symptoms(self, input_anno_text) -> List[str]:\n",
        "      \"\"\"\n",
        "      The method extract the symptom phrases from the input\n",
        "      \"\"\"\n",
        "      extractions, text = input_anno_text\n",
        "      span_extract = []\n",
        "      for extract in extractions:\n",
        "        if 'problem' in extract['entity']:\n",
        "          span_extract.append((extract['start'], extract['end']))\n",
        "\n",
        "      # Check if this span_extract is empty\n",
        "      if not span_extract:\n",
        "        return []\n",
        "      \n",
        "      span_st = span_extract[0][0]\n",
        "      final_span = []\n",
        "      final_end = span_extract[0][1]\n",
        "      for idx, (st, end) in enumerate(span_extract):\n",
        "        if idx == 0:\n",
        "          continue\n",
        "        if st - span_extract[idx-1][1] <= 2:\n",
        "          final_end = end\n",
        "          if idx == len(span_extract) - 1:\n",
        "            final_span.append((span_st, final_end))\n",
        "        else:\n",
        "          final_span.append((span_st, final_end))\n",
        "          span_st = st\n",
        "          final_end = end\n",
        "\n",
        "      text_extracts = [text[st:end].replace(\"\\n\", \" \") for st, end in final_span]\n",
        "      return text_extracts\n",
        "\n",
        "    # TODO: adding some basic checks again need to make it better.\n",
        "    def sanitize_words(self, sentence):\n",
        "      return [w for w in sentence if w not in eng_stop_words and not w.isdigit()]\n",
        "\n",
        "    def __iter__(self):\n",
        "        for idx in range(len(self.dframe)):\n",
        "          symptoms = \" \".join(self.extract_symptoms(self.data[idx]))\n",
        "          text = self.sanitize_text(symptoms)\n",
        "          yield self.sanitize_words(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_dataset(dataset):\n",
        "  seq_lengths = list()\n",
        "\n",
        "  for idx in range(len(dataset)):\n",
        "    seq_lengths.append(len(dataset[idx]))\n",
        "  max_seq_length = max(seq_lengths)\n",
        "  data_point_len = max(len(elem[0]) for elem in dataset if elem)\n",
        "  padded_dataset = torch.zeros([len(dataset), max_seq_length, data_point_len], \n",
        "                              dtype=torch.float)\n",
        "  for i in range(len(dataset)):\n",
        "    for j in range(len(dataset[i])):\n",
        "      padded_dataset[i][j] = torch.FloatTensor(dataset[i][j])\n",
        "  \n",
        "  return padded_dataset"
      ],
      "metadata": {
        "id": "81XPts3oIlGR"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Filtering and Tranformation"
      ],
      "metadata": {
        "id": "n1xDk_pMlBPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_icd_codes = pd.read_csv(\n",
        "    \"/content/drive/My Drive/DLH Final Project/mimic3/DIAGNOSES_ICD.csv\")"
      ],
      "metadata": {
        "id": "0DQMAoiAPJyh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get top #50 ICD9 codes "
      ],
      "metadata": {
        "id": "wIKf6j0HArw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = df_icd_codes[\"ICD9_CODE\"].value_counts().head(50)\n",
        "top_icd_codes = counts.index.to_list()"
      ],
      "metadata": {
        "id": "jNY4o2-_PsJD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter data to include admimission with top 50 diseases only and group and reorganize data in the following format <subject_id, hadm_id, [icd_code1, icd_code2 ...]>"
      ],
      "metadata": {
        "id": "3wSpzZYNpeIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_admissions_with_top_diseases = \\\n",
        "df_icd_codes[df_icd_codes[\"ICD9_CODE\"].isin(top_icd_codes)]\n",
        "\n",
        "df_admissions_with_top_diseases = \\\n",
        "df_admissions_with_top_diseases.groupby(\n",
        "['SUBJECT_ID', 'HADM_ID'])['ICD9_CODE'].apply(\n",
        "        list).to_frame().reset_index()"
      ],
      "metadata": {
        "id": "zCBDPURVB2C5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now select discharge summaries for the admimissions that contain atleast one of the top #50 ICD codes."
      ],
      "metadata": {
        "id": "5lLEQXITBIyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset = pd.merge(df, df_admissions_with_top_diseases, \n",
        "                       on=[\"SUBJECT_ID\", \"HADM_ID\"])\n",
        "\n",
        "df_dataset = df_dataset[df_dataset[\"CATEGORY\"] \n",
        "                          == 'Discharge summary'].reset_index()\n",
        "# free up some memory\n",
        "# del df"
      ],
      "metadata": {
        "id": "BGIpZW0wfKaO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJmeYMT3xiTE",
        "outputId": "2091bc3c-c1ff-403e-a604-2d6f3bdaa439"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55988"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to covert the ICD9 column to multi-hot encoding, we keep the old column with list of codes and added and new column with multi-hot encoding representation."
      ],
      "metadata": {
        "id": "hj5A2EJRqEzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_top_icd_codes = sorted(top_icd_codes)\n",
        "icd_code_to_idx = dict((k, v) for v, k in enumerate(sorted_top_icd_codes))"
      ],
      "metadata": {
        "id": "n-MxiQuQTZWm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new col to be added to dataframe\n",
        "multi_hot_ecoding_col = list()\n",
        "for idx in range(len(df_dataset)):\n",
        "  icd_codes = df_dataset.iloc[idx]['ICD9_CODE']\n",
        "  encoding = [0] * 50\n",
        "  for code in icd_codes:\n",
        "    encoding[icd_code_to_idx[code]] = 1    \n",
        "  multi_hot_ecoding_col.append(encoding)\n",
        "\n",
        "# new add a new column with multi-hot encoding.\n",
        "df_dataset['ICD9_CODE_ENCODED'] = multi_hot_ecoding_col"
      ],
      "metadata": {
        "id": "S7vIzsO9V-63"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract symptoms from the text. Note: currently we treat all tokens as symptoms need to add all the filters discussed in the paper later. So we added a column called \"SYMPTOMS\" which is simply tokenized form of \"TEXT\" after basic sanitization."
      ],
      "metadata": {
        "id": "rAtNNVUmCdw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "sgen = MySentences(df_dataset)\n",
        "symptom_col = list()\n",
        "for s in tqdm(sgen):\n",
        "  symptom_col.append(s)\n",
        "\n",
        "# add the new column to the dataset.\n",
        "df_dataset[\"SYMPTOMS\"] = symptom_col"
      ],
      "metadata": {
        "id": "GFO96uEPsVC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e1fa84-f695-468e-afb3-b56cfc5e1f83"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "55988it [00:16, 3332.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset = df_dataset[df_dataset.apply(lambda x: len(x.SYMPTOMS) > 0, axis=1)]"
      ],
      "metadata": {
        "id": "6jknfAsJHLe8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset = df_dataset.reset_index()"
      ],
      "metadata": {
        "id": "BtwgkSqqgXnR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset.to_csv(\"/content/drive/My Drive/DLH Final Project/kbr_df_dataset.csv\")"
      ],
      "metadata": {
        "id": "PNrtLKqTgoeH"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset = pd.read_csv(\"/content/drive/My Drive/DLH Final Project/kbr_df_dataset.csv\")"
      ],
      "metadata": {
        "id": "kB0uUo0Iixyh"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del sgen"
      ],
      "metadata": {
        "id": "ML6H3dVtLIM6"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Word2Vec Embeddings"
      ],
      "metadata": {
        "id": "RIwaO1C1td27"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg-GmTrg0i6h"
      },
      "source": [
        "Word2Vec training using gensim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xElMsqPx0sc9"
      },
      "outputs": [],
      "source": [
        "# NOTE: commenting this part so that we dont run this by mistake.\n",
        "\n",
        "# import gensim\n",
        "# sgen = MySentences(df_dataset) # a memory-friendly iterator\n",
        "# model = gensim.models.Word2Vec(sgen, min_count=5, workers=4, sample=1e-05)\n",
        "# model.save(\"/content/drive/My Drive/DLH Final Project/mimic3/word2vec-4.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct dataset with Word2Vec embeddings"
      ],
      "metadata": {
        "id": "crLZigFVcbeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load('/content/drive/My Drive/DLH Final Project/mimic3/word2vec-4.model')"
      ],
      "metadata": {
        "id": "CE5ABBKOcw21"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "X_word2vec = list()\n",
        "for idx in tqdm(range(len(df_dataset))):\n",
        "  # ignore words in not vocabulary\n",
        "  symptoms = df_dataset[\"SYMPTOMS\"][idx]\n",
        "  symptoms_emb = [model.wv[s] for s in symptoms if s in model.wv]\n",
        "  X_word2vec.append(symptoms_emb)\n",
        "\n",
        "# pad the dataset.\n",
        "# X_word2vec = pad_dataset(X_word2vec)"
      ],
      "metadata": {
        "id": "YQu3AEBEfbF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ef4175-34d2-46c6-fa1a-0b5204acba81"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54939/54939 [00:06<00:00, 8084.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# pfile = open(\"/content/drive/My Drive/DLH Final Project/X_word2vec_kbr\", \"ab\")\n",
        "# pickle.dump(X_word2vec, pfile)\n",
        "# pfile.close()"
      ],
      "metadata": {
        "id": "QTY-rPP6l2zJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct data with TF-IDF encoding"
      ],
      "metadata": {
        "id": "V8Q5MAs2xjDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "vocab_size = len(model.wv)\n",
        "tf = np.zeros((len(model.wv), len(top_icd_codes)))\n",
        "\n",
        "\n",
        "for idx in range(len(df_dataset)):\n",
        "  # XXX: TODO currently we treat all tokens from \"TEXT\" as sypmtoms\n",
        "  # get the icd codes for this discharge summary\n",
        "  symptoms = df_dataset['SYMPTOMS'][idx]\n",
        "  icd_codes = df_dataset['ICD9_CODE'][idx]\n",
        "  # create a cross product of symptoms and icd codes\n",
        "  # and update tf matrix. tf matrix keeps count of how many \n",
        "  # (i.e frequency) times <symptom, icd code> pair occur in our dataset.\n",
        "  for pair in itertools.product(symptoms, icd_codes):\n",
        "    # update count of each (symptom, icd_code) pair to compute TF\n",
        "    if pair[0] in model.wv:\n",
        "      tf[model.wv.get_index(pair[0])][icd_code_to_idx[pair[1]]] += 1\n",
        "\n",
        "# Complete the TF-IDF matrix computation.\n",
        "# Compute the number of ICD Codes (i.e diseaes) each \n",
        "# symptom is associated with.\n",
        "D_i = np.sum(tf > 0, axis=1)\n",
        "print(D_i.shape)\n",
        "\n",
        "log_N_Di = np.log(len(top_icd_codes)/D_i)\n",
        "tf_idf = (tf.T * log_N_Di).T\n",
        "print(tf_idf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOGw0qxSxrZ3",
        "outputId": "562d76e2-fa0e-4df1-c0ee-5aec93591d5b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64259,)\n",
            "(64259, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the X_tfidf dataset\n",
        "X_tf_idf = list()\n",
        "for idx in range(len(df_dataset)):\n",
        "  symptoms = df_dataset[\"SYMPTOMS\"][idx]\n",
        "  # get tf-idf vector for each symptom\n",
        "  # ignore words in not vocabulary\n",
        "  symptoms_tf_idf = [tf_idf[model.wv.get_index(s)] \\\n",
        "                     for s in symptoms if s in model.wv]\n",
        "  X_tf_idf.append(symptoms_tf_idf)\n",
        "\n",
        "# pad the dataset.\n",
        "# X_tf_idf = pad_dataset(X_tf_idf)"
      ],
      "metadata": {
        "id": "W-9C5XBuiOJ3"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# pfile = open(\"/content/drive/My Drive/DLH Final Project/X_tf_idf\", \"ab\")\n",
        "# pickle.dump(tf_idf, pfile)\n",
        "# pfile.close()"
      ],
      "metadata": {
        "id": "E3afwJciiW59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct Y (Multihot Encoding)"
      ],
      "metadata": {
        "id": "ckHf55_ylpmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-hot encoding for ICD codes diagnosed.\n",
        "y = df_dataset['ICD9_CODE_ENCODED'].to_list()"
      ],
      "metadata": {
        "id": "qwrW5WOPkBPd"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_word2vec))\n",
        "print(len((X_tf_idf)))\n",
        "print(len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP4e2hJU4S6B",
        "outputId": "ed67ba4f-8b84-4cae-da09-8707ecde2704"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54939\n",
            "54939\n",
            "54939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "D0madEdCl7w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(data):\n",
        "  x_w2v, x_tidf, y_batch = zip(*data)\n",
        "  x_w2v = pad_dataset(x_w2v).cuda()\n",
        "  x_tidf = pad_dataset(x_tidf).cuda()\n",
        "  y_batch = torch.FloatTensor(y_batch).cuda()\n",
        "  # print(f\"{x_w2v.shape}, {x_tidf.shape}, {y_batch.shape}\")\n",
        "  return x_w2v, x_tidf, y_batch\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X_w2v, X_tfidf, y):              \n",
        "    self.X_w2v = X_w2v\n",
        "    self.X_tfidf = X_tfidf\n",
        "    self.y = y\n",
        "    \n",
        "  def __len__(self):                \n",
        "    return len(self.y)\n",
        "    \n",
        "  def __getitem__(self, index):          \n",
        "    # your code here\n",
        "    return self.X_w2v[index], self.X_tfidf[index], self.y[index]\n",
        "\n",
        "dataset = CustomDataset(X_word2vec, X_tf_idf, y)\n",
        "\n",
        "split = int(len(dataset)*0.8)\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, test_dataset = random_split(dataset, lengths)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128, \n",
        "                          collate_fn=collate_fn)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=128, \n",
        "                         collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "O5HoLixIdZq_"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "yilnzXZgi8RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, output_dim):   \n",
        "    super(BiLSTM, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size=input_dim, \n",
        "                        hidden_size=embedding_dim,\n",
        "                        num_layers=1,\n",
        "                        bidirectional=True,\n",
        "                        batch_first=True)\n",
        "    \n",
        "    self.linear = nn.Linear(embedding_dim*2, \n",
        "                            output_dim)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    out, (hn, cn) = self.lstm(X)    \n",
        "    emb = torch.mean(out, dim=1)\n",
        "    output = torch.sigmoid(self.linear(emb))\n",
        "    return output"
      ],
      "metadata": {
        "id": "RpByc-wrjQ6A"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiseasePredictionModel(nn.Module):\n",
        "  def __init__(self, weight=0.4):    \n",
        "    super(DiseasePredictionModel, self).__init__()\n",
        "    self.weight = 0.4    \n",
        "    self.w2v_lstm = BiLSTM(input_dim=100, embedding_dim=50, output_dim=50)\n",
        "    self.tf_idf_lstm = BiLSTM(input_dim=50, embedding_dim=50, output_dim=50)\n",
        "  \n",
        "  def forward(self, X_w2v, X_tidf):\n",
        "    pred1 = self.w2v_lstm(X_w2v)\n",
        "    pred2 = self.tf_idf_lstm(X_tidf)\n",
        "    # compute the weighted average of predictions\n",
        "    # from the 2 models.\n",
        "    return self.weight * pred1 + (1-self.weight) * pred2"
      ],
      "metadata": {
        "id": "z6v9DR1lm34D"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "wT5_eTcAfyzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiseasePredictionModel()\n",
        "model.cuda()\n",
        "loss = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for e in range(100):\n",
        "  model.train()\n",
        "  epoc_train_loss = 0\n",
        "  for x_w2v, x_tidf, y_batch in train_loader:    \n",
        "    model.zero_grad()\n",
        "    pred = model(x_w2v, x_tidf)\n",
        "    l = loss(pred, y_batch)    \n",
        "    l.backward()\n",
        "    optimizer.step()    \n",
        "    epoc_train_loss += l.item()\n",
        "  epoc_train_loss = epoc_train_loss/len(train_loader)\n",
        "  print(f\"epoc: {e}: Train Loss: {epoc_train_loss}\")"
      ],
      "metadata": {
        "id": "djf3IOU68k9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed485d3-3102-43a1-d83e-59c3edb4043b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoc: 0: Train Loss: 0.6910153180360794\n",
            "epoc: 1: Train Loss: 0.6777748576430387\n",
            "epoc: 2: Train Loss: 0.6649659951758939\n",
            "epoc: 3: Train Loss: 0.6520988887479139\n",
            "epoc: 4: Train Loss: 0.6391024452655815\n",
            "epoc: 5: Train Loss: 0.6256635500941166\n",
            "epoc: 6: Train Loss: 0.6119832808888236\n",
            "epoc: 7: Train Loss: 0.5976034557056982\n",
            "epoc: 8: Train Loss: 0.5828218709590823\n",
            "epoc: 9: Train Loss: 0.5674722778589226\n",
            "epoc: 10: Train Loss: 0.5515962092335834\n",
            "epoc: 11: Train Loss: 0.5354359373450279\n",
            "epoc: 12: Train Loss: 0.51892232253801\n",
            "epoc: 13: Train Loss: 0.5021473158064277\n",
            "epoc: 14: Train Loss: 0.4856962183880251\n",
            "epoc: 15: Train Loss: 0.46930311741523967\n",
            "epoc: 16: Train Loss: 0.4538164575432622\n",
            "epoc: 17: Train Loss: 0.4385235850201097\n",
            "epoc: 18: Train Loss: 0.42480946739399156\n",
            "epoc: 19: Train Loss: 0.41155711145595064\n",
            "epoc: 20: Train Loss: 0.39955345376632934\n",
            "epoc: 21: Train Loss: 0.3886476997719255\n",
            "epoc: 22: Train Loss: 0.3786025263022545\n",
            "epoc: 23: Train Loss: 0.36968477685437645\n",
            "epoc: 24: Train Loss: 0.3618660885407481\n",
            "epoc: 25: Train Loss: 0.3545679463030294\n",
            "epoc: 26: Train Loss: 0.3481693138736625\n",
            "epoc: 27: Train Loss: 0.3423281334513842\n",
            "epoc: 28: Train Loss: 0.3371793677120708\n",
            "epoc: 29: Train Loss: 0.3324395468415216\n",
            "epoc: 30: Train Loss: 0.3282144395417945\n",
            "epoc: 31: Train Loss: 0.32419320221903714\n",
            "epoc: 32: Train Loss: 0.3206695560799089\n",
            "epoc: 33: Train Loss: 0.3174157424201799\n",
            "epoc: 34: Train Loss: 0.31445933731142867\n",
            "epoc: 35: Train Loss: 0.3116422001012536\n",
            "epoc: 36: Train Loss: 0.309124841506398\n",
            "epoc: 37: Train Loss: 0.3066532574594021\n",
            "epoc: 38: Train Loss: 0.3045443229723808\n",
            "epoc: 39: Train Loss: 0.30247455748707747\n",
            "epoc: 40: Train Loss: 0.30067231610070827\n",
            "epoc: 41: Train Loss: 0.2989901743482712\n",
            "epoc: 42: Train Loss: 0.2974193269430205\n",
            "epoc: 43: Train Loss: 0.29597451789088025\n",
            "epoc: 44: Train Loss: 0.29462365944718205\n",
            "epoc: 45: Train Loss: 0.29344584662900414\n",
            "epoc: 46: Train Loss: 0.2922396443264429\n",
            "epoc: 47: Train Loss: 0.2912596992628519\n",
            "epoc: 48: Train Loss: 0.2903374760996464\n",
            "epoc: 49: Train Loss: 0.28948197123962777\n",
            "epoc: 50: Train Loss: 0.2887127015653045\n",
            "epoc: 51: Train Loss: 0.28801994580169055\n",
            "epoc: 52: Train Loss: 0.2873390550010426\n",
            "epoc: 53: Train Loss: 0.28674087751396865\n",
            "epoc: 54: Train Loss: 0.28619655377643055\n",
            "epoc: 55: Train Loss: 0.2857619598507881\n",
            "epoc: 56: Train Loss: 0.28533214876471563\n",
            "epoc: 57: Train Loss: 0.2848864826698636\n",
            "epoc: 58: Train Loss: 0.28452686514965325\n",
            "epoc: 59: Train Loss: 0.28421609900718514\n",
            "epoc: 60: Train Loss: 0.28385856158511585\n",
            "epoc: 61: Train Loss: 0.2835799505890802\n",
            "epoc: 62: Train Loss: 0.2833048018172037\n",
            "epoc: 63: Train Loss: 0.28317586046665216\n",
            "epoc: 64: Train Loss: 0.282955409742372\n",
            "epoc: 65: Train Loss: 0.2827042026748491\n",
            "epoc: 66: Train Loss: 0.2825293653579645\n",
            "epoc: 67: Train Loss: 0.2824472508631473\n",
            "epoc: 68: Train Loss: 0.28227660033938495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred, y_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTJZ2QTAi_q-",
        "outputId": "c1768b3d-040d-4af8-eeb0-878371522f00"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0729, 0.0968, 0.1757,  ..., 0.0542, 0.0742, 0.0504],\n",
              "         [0.0547, 0.0849, 0.1845,  ..., 0.0453, 0.0660, 0.0379],\n",
              "         [0.0703, 0.1011, 0.1832,  ..., 0.0584, 0.0742, 0.0514],\n",
              "         ...,\n",
              "         [0.0809, 0.1038, 0.1795,  ..., 0.0606, 0.0802, 0.0565],\n",
              "         [0.0669, 0.0900, 0.1789,  ..., 0.0518, 0.0723, 0.0434],\n",
              "         [0.0598, 0.0875, 0.1804,  ..., 0.0470, 0.0698, 0.0399]],\n",
              "        device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_batch[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXWC5ISYjBRQ",
        "outputId": "2e7ec84b-da2a-4a0b-954d-ca07bbb9b658"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zX67nyLejJ8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS598_Deep_Learning_For_Healthcare_Final_Project.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}